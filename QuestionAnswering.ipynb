{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Load Context and Questions"
      ],
      "metadata": {
        "id": "MEEV_C5KqMX5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jMeMALtBVy1p"
      },
      "outputs": [],
      "source": [
        "#ChatGPT\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/CapStone/context.txt', \"r\") as file:\n",
        "    file_content = [line.strip() for line in file]\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "questionSet = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/CapStone/questions.csv')\n",
        "questiones = questionSet.values.tolist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load transformers"
      ],
      "metadata": {
        "id": "X-xKybVXqKIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "db_question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')\n",
        "\n",
        "\n",
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
        "\n",
        "\n",
        "#GPT2\n",
        "#https://huggingface.co/anas-awadalla/gpt-2-small-squad\n",
        "model_name = \"anas-awadalla/gpt-2-large-squad\"\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "#gpt2_question_answer = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
        "\n",
        "\n",
        "#T5 textgen\n",
        "#https://huggingface.co/lmqg/t5-small-squad-qag\n",
        "t5_question_answer = pipeline(\"text2text-generation\", \"lmqg/t5-small-squad-qag\")\n",
        "\n",
        "#Bert Squad\n",
        "#https://huggingface.co/deepset/bert-base-cased-squad2\n",
        "model_name = \"deepset/bert-base-cased-squad2\"\n",
        "\n",
        "bert_squad_question_answer = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
        "\n",
        "\n",
        "#Distil Bert\n",
        "#https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad\n",
        "db_question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')\n",
        "\n",
        "\n",
        "#Bert Base\n",
        "#https://huggingface.co/google-bert/bert-base-uncased\n",
        "model_name = \"bert-base-cased\"\n",
        "\n",
        "bert_base_question_answer = pipeline('question-answering', model=model_name)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtNauvoAboOR",
        "outputId": "cce81f39-1364-44be-c572-59f029fb943b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of GPT2ForQuestionAnswering were not initialized from the model checkpoint at anas-awadalla/gpt-2-large-squad and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at deepset/bert-base-cased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Responses"
      ],
      "metadata": {
        "id": "Ywj5jXJtqFQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_responses = []\n",
        "t5_responses = []\n",
        "bert_squad_responses = []\n",
        "bert_base_responses = []\n",
        "db_responses = []\n",
        "\n",
        "for question in questiones:\n",
        "  question_idx = question[0]\n",
        "  context = file_content[question_idx]\n",
        "  question_ask = question[1]\n",
        "  answer = question[2]\n",
        "\n",
        "  db_responses.append([question_ask, answer, db_question_answerer(question=question_ask, context=context)['answer']])\n",
        "  #gpt2_responses.append([question_ask, answer, gpt2_question_answer(question=question_ask, context = context)['answer']])\n",
        "  #t5_responses.append([question_ask, answer, t5_question_answer(f\"{question_ask}, context = {context}\")[0]['generated_text']])\n",
        "  bert_squad_responses.append([question_ask, answer, bert_squad_question_answer({'question': question_ask, 'context': context})['answer']])\n",
        "  bert_base_responses.append([question_ask, answer, bert_base_question_answer({'question': question_ask, 'context': context})['answer']])\n",
        "\n",
        "pd.DataFrame(db_responses).to_csv('/content/drive/MyDrive/Colab Notebooks/CapStone/db_responses.csv')\n",
        "#pd.DataFrame(gpt2_responses).to_csv('/content/drive/MyDrive/Colab Notebooks/CapStone/gpt2_responses.csv')\n",
        "#pd.DataFrame(t5_responses).to_csv('/content/drive/MyDrive/Colab Notebooks/CapStone/t5_responses.csv')\n",
        "pd.DataFrame(bert_squad_responses).to_csv('/content/drive/MyDrive/Colab Notebooks/CapStone/bert_squad_responses.csv')\n",
        "pd.DataFrame(bert_base_responses).to_csv('/content/drive/MyDrive/Colab Notebooks/CapStone/bert_base_responses.csv')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEPsLjv_FUOf",
        "outputId": "24e66a8b-6f20-4a3c-cf0e-ae45f01682c1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosine Similarities"
      ],
      "metadata": {
        "id": "dL1IveOSqCB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(pratikchakra18)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Sample documents\n",
        "\n",
        "# Create TF-IDF features\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Compute Cosine Similarity\n",
        "\n",
        "db_responses = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/CapStone/db_responses.csv').values.tolist()\n",
        "bert_squad_responses = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/CapStone/bert_squad_responses.csv').values.tolist()\n",
        "bert_base_responses = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/CapStone/bert_base_responses.csv').values.tolist()\n",
        "\n",
        "for i in range(len(db_responses)):\n",
        "  answers = tfidf_vectorizer.fit_transform((str(db_responses[i][1]), str(db_responses[i][2])))\n",
        "  if str(db_responses[i][2]) == str(db_responses[i][3]):\n",
        "    db_responses[i][5] = 1\n",
        "  else:\n",
        "    db_responses[i][5] = cosine_similarity(answers[0:1], answers)[0][1]\n",
        "\n",
        "for i in range(len(bert_squad_responses)):\n",
        "  answers = tfidf_vectorizer.fit_transform([str(bert_squad_responses[i][1]), str(bert_squad_responses[i][2])])\n",
        "  if str(bert_squad_responses[i][1]) == str(bert_squad_responses[i][2]):\n",
        "    bert_squad_responses[i][3] = 1\n",
        "  else:\n",
        "    bert_squad_responses[i][3] = cosine_similarity(answers[0:1], answers)[0][1]\n",
        "\n",
        "for i in range(len(bert_base_responses)):\n",
        "  answers = tfidf_vectorizer.fit_transform([str(bert_base_responses[i][1]), str(bert_base_responses[i][2])])\n",
        "  if str(bert_base_responses[i][1]) == str(bert_base_responses[i][2]):\n",
        "    bert_base_responses[i][3] = 1\n",
        "  else:\n",
        "    bert_base_responses[i][3] = cosine_similarity(answers[0:1], answers)[0][1]"
      ],
      "metadata": {
        "id": "Zs8L9BZhTJ2O"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(db_responses).to_csv('/content/drive/MyDrive/Colab Notebooks/CapStone/db_responses.csv')\n",
        "pd.DataFrame(bert_squad_responses).to_csv('/content/drive/MyDrive/Colab Notebooks/CapStone/bert_squad_responses.csv')\n",
        "pd.DataFrame(bert_base_responses).to_csv('/content/drive/MyDrive/Colab Notebooks/CapStone/bert_base_responses.csv')"
      ],
      "metadata": {
        "id": "rwGw8gODcDsL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Individual models running on sample question"
      ],
      "metadata": {
        "id": "98zQNx6op6sH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "db_question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')\n",
        "\n",
        "context = file_content[2]\n",
        "question = \"What is southern California often abbreviated as?\"\n",
        "result = db_question_answerer(question=question, context=context)\n",
        "print(result['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Gibj94FFnOb",
        "outputId": "78773b9c-c1b3-40e9-e6de-4246decffd84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SoCal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://huggingface.co/anas-awadalla/gpt-2-small-squad\n",
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
        "\n",
        "model_name = \"anas-awadalla/gpt-2-large-squad\"\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "qa = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
        "\n",
        "question = \"What is southern California often abbreviated as?\"\n",
        "qa(context = file_content[2], question = question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7kawsaEj_Jy",
        "outputId": "43b398c7-c30d-4010-89c6-ea3cedd72e1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForQuestionAnswering were not initialized from the model checkpoint at anas-awadalla/gpt-2-large-squad and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of GPT2ForQuestionAnswering were not initialized from the model checkpoint at anas-awadalla/gpt-2-large-squad and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.00038174836663529277,\n",
              " 'start': 7683,\n",
              " 'end': 7703,\n",
              " 'answer': ' of issuing stay-at-'}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://huggingface.co/lmqg/t5-small-squad-qag\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text2text-generation\", \"lmqg/t5-small-squad-qag\")\n",
        "output = pipe(f\"{question}, context = {file_content[2]}\")[0]['generated_text']\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dn-kkXWW89Sc",
        "outputId": "6d61083c-dbdc-45e2-98e3-faa37e77ff18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (4645 > 512). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Approximately one-third of California's population is Southern California?, | | Southern California\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://huggingface.co/deepset/bert-base-cased-squad2\n",
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
        "\n",
        "model_name = \"deepset/bert-base-cased-squad2\"\n",
        "question = \"What is southern California often abbreviated as?\"\n",
        "# a) Get predictions\n",
        "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
        "QA_input = {\n",
        "    'question': question,\n",
        "    'context': file_content[2]\n",
        "}\n",
        "res = nlp(QA_input)\n",
        "print(res['answer'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3zYUJ1j-plc",
        "outputId": "f9900669-5326-4e27-bb26-83f60f91af84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/bert-base-cased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SoCal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://huggingface.co/google-bert/bert-base-uncased\n",
        "model_name = \"bert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "question = \"What is southern California often abbreviated as?\"\n",
        "# a) Get predictions\n",
        "nlp = pipeline('question-answering', model=model_name)\n",
        "QA_input = {\n",
        "    'question': question,\n",
        "    'context': file_content[2]\n",
        "}\n",
        "res = nlp(QA_input)\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wM9gRZVHDD8",
        "outputId": "f5fee5c0-a08e-404a-f59b-59c339a3f1a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.00021273687889333814, 'start': 21142, 'end': 21164, 'answer': '000°N 117.000°W\\ufeff / 34.'}\n"
          ]
        }
      ]
    }
  ]
}